
# ğŸ¤– Agentic RAG Demo App

Build an **Agentic Retrieval-Augmented Generation (RAG)** pipeline using **CrewAI**, **LangChain**, and **Gradio** â€” inspired by the blog *â€œHands-on demo with building Agentic RAG pipelineâ€* by Ajay Arunachalam.

---

## ğŸš€ Overview

This project demonstrates how to build an **LLM-powered Agentic RAG pipeline** where an **agent** decides which data source to query before generating an answer â€” improving accuracy and contextual relevance.

---

## ğŸ§© Features

- ğŸ” Multi-source document retrieval  
- ğŸ§  Agentic decision-making using LLMs  
- ğŸ’¬ Context-aware response generation  
- ğŸŒ Simple and interactive Gradio UI  
- ğŸ§° Uses LangChain, CrewAI, and Tavily for retrieval & reasoning  

---

## âš™ï¸ Tech Stack

- **CrewAI**  
- **CrewAI Tools**  
- **LangChain Community**  
- **LangChain Groq**  
- **LangChain HuggingFace**  
- **Sentence Transformers**  
- **Gradio**  
- **Tavily Python**

---

## ğŸ“¦ Installation

1. Clone the repository:
   ```bash
   git clone <repo-url>
   cd <repo-directory>
````

2. Create and activate a virtual environment:

   ```bash
   python -m venv venv
   source venv/bin/activate  # macOS/Linux
   venv\Scripts\activate     # Windows
   ```

3. Install dependencies:

   ```bash
   pip install -r requirements.txt
   ```

---

## ğŸ§° Requirements

Your `requirements.txt` should look like this:

```
crewai
crewai_tools
langchain_community
sentence-transformers
langchain-groq
langchain_huggingface
gradio
tavily-python
```

---

## ğŸš¦ How to Run

1. Add your API keys in a `.env` file (for OpenAI, Tavily, etc.)
2. Run the application:

   ```bash
   python agentic_rag_app_demo.py
   ```
3. Open the **Gradio UI** link shown in your terminal.

---

## ğŸ§  How It Works

1. User asks a question via the Gradio interface.
2. The **CrewAI agent** decides which data source or retriever to use.
3. Relevant context is fetched and passed to the LLM.
4. The **LLM** generates a detailed, contextually accurate response.

---



